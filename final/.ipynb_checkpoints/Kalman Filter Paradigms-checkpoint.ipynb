{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter Associative Learning Paradigms\n",
    "\n",
    "### kalman.py documentation\n",
    "\n",
    "`KalmanFilter()` object \n",
    "* arguments:\n",
    "\n",
    "    * `d_in` : $x_n \\in R^{d_{in}}$ number of conditioned stimuli (no default)\n",
    "    * `var_w` : $\\sigma_w^2$ initial weight variance (default $1$)\n",
    "    * `var_r` : $\\sigma_r^2$ initial reward variance (default $1$)\n",
    "    * `volatility` : $\\tau^2$ diffusion constant (default $0.01$)\n",
    "    * `rate` : learning rate $\\eta$ for $\\tau^2$ and $\\sigma^2$ in the extended model (default is 0 $\\implies$ no updates).\n",
    "\n",
    "* methods:\n",
    "    * `update(x, reward)` : updates parameters when observation of `x` stimulus vector followed by `reward`\n",
    "    * `zero()` : forget everything it learned\n",
    "    * `train(x, reward, nepochs=10)` : `nepochs` trials of `update(x, reward)`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How $\\sigma_r^2$ and $\\tau^2$ fit into Kalman Filter model\n",
    "\n",
    "* $\\sigma_r^2$ reflects how noisy you believe the underlying reward emission distribution $p(r_n|v_n)$ is. Higher $\\sigma_r^2$ means you believe $v_n$ is not as good a predictor of the reward. The amount you learn (as measured by the magnitude of the learning rate) is inversely related to $\\sigma_r^2$. \n",
    "\n",
    "* Essentially, if you believe the reward distribution is inherently noisy, each cue association has less meaning to you. \n",
    "\n",
    "* Learning rate is higher/lower when $\\sigma_r^2$ is lower/higher.\n",
    "\n",
    "* $\\tau^2$ reflects how noisy you believe the state transitions $p(w_n | w_{n - 1})$ are. Higher $\\tau^2$ means that you believe the world is changing pretty quickly, so you should learn new associations more quickly. \n",
    "\n",
    "* So learning rate is higher/lower when $\\tau^2$ is higher/lower.\n",
    "\n",
    "* These parameters represent an agent's theory about the state of the world's dynamics.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "\n",
    "### MLL gradient update rules for $\\sigma_r^2$ and $\\tau^2$\n",
    "\n",
    "* Rules:\n",
    "\n",
    "    * $\\sigma_r^2 := \\sigma_r^2 + \\eta \\Delta \\sigma_r^2$\n",
    "    \n",
    "    * $\\tau^2 := \\tau^2 + \\eta \\Delta \\tau^2$\n",
    "\n",
    "    * $\\Delta \\sigma_r^2 = \\frac{1}{2 \\sigma_r^2} \\left(\\frac{\\delta_n^2}{\\sigma_r^2} - 1\\right)$\n",
    "\n",
    "    * $\\Delta \\tau^2 = \\frac{1}{2 \\tau^2} \\left(\\frac{|w_n - w_{n - 1}|^2}{\\tau^2} - d_{in}\\right)$\n",
    "    \n",
    "    * $\\eta$ is just a learning rate for these parameters\n",
    "    \n",
    "* Intuition:\n",
    "\n",
    "    * Change in a variance parameter is proportional to the product of the reciprocal variance and the observed \"relative error\" in the variance parameter.\n",
    "    \n",
    "    * Key point:\n",
    "    \n",
    "        * $\\delta_n^2$ is a single point estimate of $\\sigma_r^2$ in that $\\mathbb{E}[\\delta_n^2] = \\sigma_r^2$ if $\\sigma_r^2$ is correct. \n",
    "    \n",
    "        * $\\frac{|w_n - w_{n - 1}|}{d_{in}}^2$ is a single point estimate of $\\tau^2$ in that $\\mathbb{E}[|w_n - w_{n - 1}|^2] = d_{in}\\tau^2$ if $\\tau^2$ is correct.\n",
    "        \n",
    "    * If the single point estimate of the variance exceeds your overall estimate of the variance, you should increase your estimate. If your single point estimate of the variance is less than your overall estimate of the variance, you should decrease your estimate. If they are equal there's nothing to learn.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we care:\n",
    "\n",
    "* Modulates learning rate based on outcomes.\n",
    "    \n",
    "* $\\tau^2$ and $\\sigma_r^2$ represent beliefs about the dynamics of the world, which should be updated when confronted with new data. These updates allow me to both understand what the world is like and update my understanding when the world changes.\n",
    "    \n",
    "* My preliminary observations: \n",
    "    * $\\eta = 0.1$ is an OK value. \n",
    "    \n",
    "    * Latent inhibition lasts for shorter periods of time. This makes sense because once you understand that the world has changed, you throw out old beliefs and acquire new knowledge more quickly.\n",
    "    \n",
    "    * In general learning seems to be faster and more adaptable to changes.\n",
    "    \n",
    "    * Recovery phenomenon are faster as well. Same underlying principle.\n",
    "    \n",
    "    * There are probably errors in my implementation, so we should test it extensively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single CS Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kalman import KalmanFilter\n",
    "import numpy as np\n",
    "\n",
    "KF = KalmanFilter(1,\n",
    "                var_w=1,\n",
    "                var_r=1,\n",
    "                volatility=0.01,\n",
    "                rate=0.1)\n",
    "A = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Basic Association $A \\rightarrow +; A \\rightarrow ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to A: 1.000\n"
     ]
    }
   ],
   "source": [
    "KF.train(A, 1, nepochs=10)\n",
    "    \n",
    "print \"Response to A: %.3f\" % KF.predict(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Latent Inhibition $A \\rightarrow - ; A \\rightarrow +; A \\rightarrow ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to A w/o preconditioning: 0.996\n",
      "Response to A w/ preconditioning: 0.902\n"
     ]
    }
   ],
   "source": [
    "KF.zero()\n",
    "KF.train(A, 1, nepochs=2)\n",
    "\n",
    "print (\"Response to A w/o preconditioning: %.3f\" % KF.predict(A))\n",
    "\n",
    "KF.zero()\n",
    "KF.train(A, 0, nepochs=2)\n",
    "KF.train(A, 1, nepochs=2)\n",
    "\n",
    "print (\"Response to A w/ preconditioning: %.3f\" % KF.predict(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two CS setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KF = KalmanFilter(2, var_w=1, var_r=1, volatility=0.01, rate=0.1)\n",
    "\n",
    "A = np.array([1, 0])\n",
    "B = np.array([0, 1])\n",
    "AB = np.array([1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overshadowing Extinction $AB \\rightarrow +; A \\rightarrow -; A \\rightarrow ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to B after overshadowing w/o A extinction: 0.500\n",
      "Response to B after overshadowing w/ A extinction: 0.909\n"
     ]
    }
   ],
   "source": [
    "KF.zero()\n",
    "KF.train(AB, 1, nepochs=10)\n",
    "\n",
    "print \"Response to B after overshadowing w/o A extinction: %.3f\" % KF.predict(B)\n",
    "\n",
    "KF.zero()\n",
    "\n",
    "KF.train(AB, 1, nepochs=10)\n",
    "\n",
    "KF.train(A, 0, nepochs=10)\n",
    "    \n",
    "print \"Response to B after overshadowing w/ A extinction: %.3f\" % KF.predict(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Blocking Extinction $A \\rightarrow +; AB \\rightarrow +; A \\rightarrow -; B \\rightarrow ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to B w/o A extinction 0.000\n",
      "Response to B w/ A extinction 0.845\n"
     ]
    }
   ],
   "source": [
    "KF.zero()\n",
    "KF.train(A, 1, nepochs=10)\n",
    "KF.train(AB, 1, nepochs=10)\n",
    "print \"Response to B w/o A extinction %.3f\" % KF.predict(B)\n",
    "\n",
    "KF.zero()\n",
    "KF.train(A, 1, nepochs=10)\n",
    "KF.train(AB, 1, nepochs=10)\n",
    "KF.train(A, 0, nepochs=10)\n",
    "print \"Response to B w/ A extinction %.3f\" % KF.predict(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : Add More Basic Paradigms, Create Paradigms KF fails at but KF++ doesn't fail at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
